#!/usr/bin/env python

import sys
import os
import hashlib
import json
import fnmatch
import requests
import pycurl
import dulwich.porcelain
import errno
import requests_toolbelt.multipart.encoder
import httplib
import re
import paramiko
import urlparse
import getpass
import getopt

DEFAULT_LOG_LEVELS=['INFO', 'WARN', 'ERROR']

LOG_LEVELS = DEFAULT_LOG_LEVELS

def print_transfer_string(transferred, total, filename, direction):
  if total > 1073741824:
    suffix = "GB"
    divider = 1073741824
  elif total > 1048576:
    suffix = "MB"
    divider = 1048576
  elif total > 1024:
    suffix = "KB"
    divider = 1024
  else:
    suffix = "bytes"
    divider = 1

  divided_tran = int(transferred / divider)
  divided_total = int(total / divider)
  percent = int(transferred * 100 / total)
  strlen_no_file = len(direction) + 1 + 1 + len(str(divided_tran)) + 1 + len(str(divided_total)) + 1 + len(suffix) + 2 + 3 + 2

  filenamelen = len(filename)
  if strlen_no_file + len(filename) > 80:
    filenamelen = 80 - strlen_no_file

  sys.stdout.write("{0} {1:.{2}s} {3}/{4} {5} ({6}%)".format(direction,
                                                             filename,
                                                             filenamelen,
                                                             divided_tran,
                                                             divided_total,
                                                             suffix, percent))

  sys.stdout.flush()
  sys.stdout.write("\r")

class SCP(object):
  def __init__(self, configuration):
    self.direction = "Downloading"
    self.filename = "Unset"
    self.configuration = configuration

  def _print_total(self, transferred, total):
    print_transfer_string(transferred, total, self.filename, self.direction)

  def _get_location_info_scp(self):
    parser = urlparse.urlparse(self.configuration['remote'])
    username = parser.username
    if username is None:
      username = getpass.getuser()
    return (parser.hostname, parser.path, username)

  def load(self, filename, checksum):
    log_debug("load_scp")
    (hostname, remote_dir, username) = self._get_location_info_scp()
    log_debug("hostname: %s, remote_dir: %s, username: %s" % (hostname,
                                                              remote_dir,
                                                              username))
    ssh = paramiko.SSHClient()
    ssh.load_host_keys(os.path.expanduser(os.path.join("~", ".ssh", "known_hosts")))
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh.connect(hostname, username=username)
    sftp = ssh.open_sftp()
    sftp.chdir(remote_dir)
    remotefile = '%s.got' % (checksum)
    self.direction = "Downloading"
    self.filename = filename
    sftp.get(remotefile, filename, callback=self._print_total)
    sys.stdout.write("\n")
    sftp.close()
    ssh.close()

  def store(self, filename, checksum):
    log_debug("store_scp")
    (hostname, remote_dir, username) = self._get_location_info_scp()
    log_debug("hostname: %s, remote_dir: %s, username: %s" % (hostname,
                                                              remote_dir,
                                                              username))
    ssh = paramiko.SSHClient()
    ssh.load_host_keys(os.path.expanduser(os.path.join("~", ".ssh", "known_hosts")))
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh.connect(hostname, username=username)
    sftp = ssh.open_sftp()
    sftp.chdir(remote_dir)
    remotefile = '%s.got' % (checksum)

    # here we do an optimization; if the remote file with the right filename
    # already exists, we don't need to upload it again.  Just get out of here
    try:
      sftp.stat(remotefile)
      log_debug("File existed on remote, skipping upload...")
      sftp.close()
      ssh.close()
      return
    except IOError:
      log_debug("Uploading file to remote...")

    self.direction = "Uploading"
    self.filename = filename
    sftp.put(filename, remotefile, callback=self._print_total)
    sys.stdout.write("\n")
    sftp.close()
    ssh.close()

  def version(self):
    return self.configuration['version']

class SRR(object):
  def __init__(self, configuration):
    self.last_mb = -1
    self.filename = "Unset"
    self.configuration = configuration
    self.upload_len = -1

  def _get_location_info_srr(self):
    result_re = re.compile('http://(.*)/.*/(\d+)$')
    matches = result_re.match(self.configuration['remote'])
    return (matches.group(1), matches.group(2))

  def upload_cb(self, monitor):
    print_transfer_string(monitor.bytes_read, self.upload_len, self.filename,
                          "Uploading")

  def store(self, filename, checksum):
    log_debug("store_srr")
    (server_name, parent_id) = self._get_location_info_srr()
    local_path = filename
    target_id = ''
    remote_path = '%s' % checksum
    description = 'Got storage for %s @ TBD hashtag' % (filename)

    # first check to see if the file already exists in the SRR
    conn = httplib.HTTPConnection(server_name)
    conn.request("GET", "/srr/api/file_metadata/sha256/%s" % checksum)
    response = conn.getresponse()
    conn.close()
    if response.status == 200:
      log_debug("File existed on remote, skipping upload...")
      return

    # if the response was 404, the object was not in the store and thus we need
    # to upload it
    log_debug("Uploading file to remote...")

    e = requests_toolbelt.multipart.encoder.MultipartEncoder(fields={
      'parent_id' : parent_id,
      'target_id' : '',
      'description' : description,
      'file' : (checksum, open(filename, 'rb'), 'application/octet-stream')})
    self.upload_len = len(e)
    self.filename = filename
    m = requests_toolbelt.multipart.encoder.MultipartEncoderMonitor(e,
                                                                    self.upload_cb)

    response = requests.post('http://%s/srr/api/add_file' % server_name, data = m, headers = {'Content-Type' : m.content_type})

    if response.status_code != 200:
      raise SRRError("%s: %s" % (response.reason, result))
    new_id_re = re.compile(r' file_id=(\d+)\s*$')
    m = new_id_re.search(response.text)
    if m:
      return int(m.group(1))
    else:
      raise SRRError("Unexpected result from SRR: %s" % result)

  def _get_remote_path_srr(self, server, checksum):
    response = requests.get('http://%s/srr/api/file_metadata/sha256/%s' % (server, checksum))
    return response.json()['url']

  def _curlprogress(self, down_total, down_current, up_total, up_current):
    if down_total == 0:
      return

    current_mb = int(down_current) / 1048576
    if current_mb > self.last_mb or down_current == down_total:
      self.last_mb = current_mb
      print_transfer_string(down_current, down_total, self.filename,
                            "Downloading")

  def load(self, filename, checksum):
    log_debug("load_srr")
    (server, parent_id) = self._get_location_info_srr()

    path = self._get_remote_path_srr(server, checksum)
    log_debug("load_srr, filename %s, path %s" % (filename, path))

    self.last_mb = -1
    self.filename = filename
    with open(filename, 'wb') as f:
      c = pycurl.Curl()
      c.setopt(c.URL, path)
      c.setopt(c.WRITEDATA, f)
      c.setopt(c.NOPROGRESS, 0)
      c.setopt(c.PROGRESSFUNCTION, self._curlprogress)
      c.perform()
      c.close()

    sys.stdout.write("\n")

  def version(self):
    return self.configuration['version']

class File(object):
  def __init__(self, configuration):
    self.configuration = configuration
    self.block_size = 8192

  def store(self, filename, checksum):
    log_debug("store_file")
    parser = urlparse.urlparse(self.configuration['remote'])
    dstpath = os.path.join(parser.path, checksum + ".got")
    if os.path.exists(dstpath):
      log_debug("File existed on remote, skipping upload...")
      return

    src = open(filename, "rb")
    dst = open(dstpath, "wb")

    total_len = os.fstat(src.fileno()).st_size
    transferred = 0

    while True:
      block = src.read(self.block_size)

      if not block:
        # end of file
        break
      else:
        dst.write(block)

      print_transfer_string(transferred, total_len, filename, "Uploading")
      transferred += self.block_size

    print_transfer_string(total_len, total_len, filename, "Uploading")
    sys.stdout.write("\n")
    dst.close()
    src.close()

  def load(self, filename, checksum):
    log_debug("load_file")
    parser = urlparse.urlparse(self.configuration['remote'])

    srcpath = os.path.join(parser.path, checksum + ".got")

    src = open(srcpath, "rb")
    dst = open(filename, "wb")

    total_len = os.fstat(src.fileno()).st_size
    transferred = 0

    while True:
      block = src.read(self.block_size)

      if not block:
        # end of file
        break
      else:
        dst.write(block)

      print_transfer_string(transferred, total_len, filename, "Downloading")
      transferred += self.block_size

    print_transfer_string(total_len, total_len, filename, "Downloading")
    sys.stdout.write("\n")
    dst.close()
    src.close()

  def version(self):
    return self.configuration['version']

def usage():
  print("git got <command> [<args>]")
  print("")
  print("The available git got commands are:")
  print("  init <type> <url>   Initialize the remote to be used with the repository,")
  print("                      where <type> is one of 'scp', 'srr', or 'file'.")
  print("  get [<file>...]     With no arguments, retrieve all remote files to the local")
  print("                      working area.  With one or more arguments, retrieve just")
  print("                      those remote files to the local working area.")
  print("  add <file>...       Add one or more files to the remote repository.  Note")
  print("                      that directories are not allowed.")
  print("  status [<file>...]  With no arguments, request the status of all got")
  print("                      tracked files.  With one or more arguments, request")
  print("                      just the status of the named files.")
  print("  reset <file>...     Overwrite one or more local got files with the remote")
  print("                      copy.  Note that directories are not allowed.")

def git_add(repo, filename):
  log_debug('git_add: Adding file %s to %s' % (filename, repo))
  dulwich.porcelain.add(repo, filename)

def file_hash(filename):
  hasher = hashlib.sha256()
  with open(filename, 'rb') as infp:
    while True:
      data = infp.read(8192)
      if not data:
        break
      hasher.update(data)
  return hasher.hexdigest()

def get_cb(repo, got_filename, real_filename):
  try:
    log_debug('get_cb: Using %s for local file' % real_filename)
    if status_local(got_filename, real_filename):
      log_debug("File already exists, and has right checksum; skipping download...")
      return
    log_debug("Downloading remote file...")
    remote_obj.load(real_filename, open(got_filename).read().rstrip())
  except Exception as e:
    log_error('Failed to retrieve file %s' % real_filename, e)

def reset_cb(repo, got_filename, real_filename):
  try:
    # FIXME: here we need to check if the local file is already unmodified, and
    # if so, do nothing
    log_debug('reset_cb: Resetting %s' % real_filename)
    if status_local(got_filename, real_filename):
      log_debug("File already exists locally, no need for work")
      return
    log_debug('reset_cb: Using %s for local got file' % got_filename)
    remote_obj.load(real_filename, open(got_filename).read().rstrip())
  except Exception as e:
    log_error('Failed to reset %s' % real_filename, e)

def add_cb(repo, got_filename, real_filename):
  try:
    log_debug('add_cb: Adding %s' % real_filename)
    csum = file_hash(real_filename)
    remote_obj.store(real_filename, csum)
    with open(got_filename, 'w') as hash_file:
      hash_file.write('%s' % csum)
    git_add(repo, got_filename)
    with open('.gitignore', 'a') as gitignorefile:
      gitignorefile.write('%s\n' % real_filename)
    git_add(repo, '.gitignore')
  except Exception as e:
    print sys.exc_traceback.tb_lineno
    log_error('Failed to add %s' % real_filename, e)

def status_local(got_filename, real_filename):
  """
  Determines if there are local changes made to the file specified in the
  filename parameter.  Assumes filename is the fully qualified name of a got
  meta file.

  @filename Got meta filename
  @return 1 if The file is unchanged
  @return 0 if the file is changed or not found
  """
  try:
    if not os.path.exists(real_filename):
      log_debug('status_local: Did not find file %s' % real_filename)
      return False
    sum1 = file_hash(real_filename)
    sum2 = open(got_filename).read().rstrip()
    if sum1 != sum2:
      log_debug('status_local: Got hash %s != file hash %s' % (sum1, sum2))
      return False
    return True
  except Exception as e:
    log_error('status_local: Failed to status %s' % real_filename, e)
    return False

def status_cb(repo, got_filename, real_filename):
  """
  Retrieves the status of the filename specified.  Invoked from the main
  walker loop.  Assumes the filename passed in is a got meta file.

  @param filename Name of the got meta file to check
  """
  try:
    if not status_local(got_filename, real_filename):
      return 'Modified: %s' % real_filename
  except Exception as e:
    log_error('Failed to status %s' % real_filename, e)

def log(level, message, exception):
  if None is exception:
    print '%s : %s' % (level, message)
  else:
    print '%s:%s:%s' % (level, message, exception)

def log_error(message, exception = None):
  if 'ERROR' in LOG_LEVELS:
    log('ERROR', message, exception)

def log_warn(message, exception = None):
  if 'WARN' in LOG_LEVELS:
    log('WARN', message, exception)

def log_info(message, exception = None):
  if 'INFO' in LOG_LEVELS:
    log('INFO', message, exception)

def log_debug(message, exception = None):
  if 'DEBUG' in LOG_LEVELS:
    log('DEBUG', message, exception)

def upgrade_cb(new):
  pass

def walker(function, repo, origpath, args):
  output = []
  log_debug("walker: args: " + str(args))
  for arg in args:
    fullpath = os.path.normpath(os.path.join(origpath, arg))
    log_debug('walker: processing argument %s' % fullpath)
    if os.path.isfile(fullpath):
      log_debug('walker: processing file %s' % fullpath)
      (base, filename) = os.path.split(fullpath)
      output.append(function(repo, os.path.join(base, '.%s.got' % filename), fullpath))
    else:
      for base, dirs, filenames in os.walk(fullpath):
        if '.git' in dirs:
          dirs.remove('.git')
        if '.got' in dirs:
          dirs.remove('.got')
        for filename in fnmatch.filter(filenames, '.*.got'):
          realpath = os.path.normpath(os.path.join(base, filename[1:-4]))
          gotpath = os.path.normpath(os.path.join(base, filename))
          log_debug('walker: processing file %s' % realpath)
          output.append(function(repo, gotpath, realpath))
  return output

remote_obj = None

def check_initialized():
  global remote_obj
  if os.path.isfile('.got/storage'):
    with open('.got/storage', 'r') as storagefp:
      configuration = json.load(storagefp)
    if configuration['remote_type'] == 'srr':
      remote_obj = SRR(configuration)
    elif configuration['remote_type'] == 'file':
      remote_obj = File(configuration)
    else:
      remote_obj = SCP(configuration)
    return True
  return False

def check_version(version):
  return VERSION == remote_obj.version()

VERSION = 1

def mkdir_p(path):
  """
  Function to make a directory and all intermediate directories as
  necessary.  The functionality differs from os.makedirs slightly, in
  that this function does *not* raise an error if the directory already
  exists.
  """
  if path is None:
    raise Exception("Path cannot be None")

  if path == '':
    # this can happen if the user did something like call os.path.dirname()
    # on a file without directories.  Since os.makedirs throws an exception
    # in that case, check for it here and allow it.
    return

  try:
    os.makedirs(path)
  except OSError as err:
    if err.errno != errno.EEXIST or not os.path.isdir(path):
      raise

def find_git_path_and_chdir():
  """
  This function is expected to be called at the beginning and goes looking
  for the closest .git subdirectory or file in the filesystem hierarchy.
  We look for the .git subdirectory by first looking in ./.git, then changing
  directory to .. and looking for .git, etc, until we either find a .git
  directory or until we hit / (at which point we raise an error).  At the end
  we have changed directory to the appropriate level, which is similar to how
  git itself operates.  Assuming we succeed, this function returns a relative
  path to the original path we started out in so that subsequent operations
  (that may git add) can do the right thing.
  """
  origpath = os.getcwd()
  curpath = origpath
  while not os.path.exists(os.path.join(curpath, '.git')) and curpath != '/':
    os.chdir("..")
    curpath = os.getcwd()

  if curpath == '/':
    raise Exception, "Could not find git repository"

  # OK, we found the .git directory/file.  Now return the original path
  # relative to that found directory
  return os.path.relpath(origpath, os.path.commonprefix([origpath, os.getcwd()]))

def main(argv):
  try:
    opts, args = getopt.gnu_getopt(argv[1:], 'd:h', ['debug', 'help'])
  except getopt.GetoptError as err:
    print(str(err))
    usage()
    return 1

  for o, a in opts:
    if o in ("-d", "--debug"):
      try:
        d_int = int(a)
      except ValueError:
        usage()
        return 1
    elif o in ("-h", "--help"):
      usage()
      return 0

  origpath = find_git_path_and_chdir()

  repo = dulwich.porcelain.open_repo(".")

  command = args[0]

  # For bootstrapping purposes we have to check if the command is "init" first.
  if command == 'init':
    if len(args) != 3:
      usage()
      return 1

    remote_type = args[1]
    if remote_type != 'srr' and remote_type != 'scp' and remote_type != 'file':
      usage()
      return 1

    remote = args[2]
    mkdir_p('.got')
    configuration = { 'remote' : remote , 'remote_type' : remote_type , 'version' : VERSION }
    with open('.got/storage', 'w') as storagefile:
      json.dump(configuration, storagefile)
    git_add(repo, '.got/storage')
    return 0

  if not check_initialized():
    print 'Got not initialized\n'
    usage()
    return 1

  # If an upgrade is required, it really needs to be before we check the
  # version or we have a chicken and egg problem.
  if command == 'upgrade':
    if len(args) != 1:
      usage()
      return 1

    upgrade_cb(VERSION)
    return 0

  if not check_version(VERSION):
    print 'Version of got repository requires upgrading, run upgrade command'
    usage()
    return 1

  if command == 'add':
    if len(args) < 1:
      usage()
      return 1

    # for add, we only allow adding files (not subdirectories); check that here
    for arg in args[1:]:
      if not os.path.isfile(os.path.join(origpath, arg)):
        print "git-got only allows files, not subdirectories, to be added"
        usage()
        return 1

    walker(add_cb, repo, origpath, args[1:])

    return 0
  elif command == 'reset':
    if len(args) < 1:
      usage()
      return 1

    # for reset, we only allow adding files (not subdirectories); check that
    # here
    for arg in args[1:]:
      if not os.path.isfile(os.path.join(origpath, arg)):
        print "git-got only allows files, not subdirectories, to be reset"
        usage()
        return 1

    walker(reset_cb, repo, origpath, args[1:])
    return 0
  elif command == 'get':
    if len(args) == 1:
      path = ['.']
    elif len(args) > 1:
      path = args[1:]
    else:
      usage()
      return 1

    walker(get_cb, repo, origpath, path)
    return 0
  elif command == 'status':
    if len(args) == 1:
      path = ['.']
    elif len(args) > 1:
      path = args[1:]
    else:
      usage()
      return 1

    changes = walker(status_cb, repo, origpath, path)
    print '# Changes',
    for change in changes:
      if None != change:
        print '\n# %s' % change,
    print '\n',
    return 0
  else:
    usage()
    return 1

if __name__ == "__main__":
  exit(main(sys.argv))

# vim: set filetype=python :
